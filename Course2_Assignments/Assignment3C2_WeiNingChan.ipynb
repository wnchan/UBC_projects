{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55ed771-87cd-4e6a-b2e5-dabb40cd3314",
   "metadata": {},
   "source": [
    "# **Assignment 3**\n",
    "**Data Science Tools and Advanced Modelling Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990005f-ce6d-4f7a-833d-b24cda73ff07",
   "metadata": {},
   "source": [
    "## Student Information\n",
    "\n",
    "**Your name (as appears on Canvas):** Wei Ning Chan\n",
    "\n",
    "**Your student ID:** 23385923"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6b6590-dfe3-4d53-90f1-0dd6605594b6",
   "metadata": {},
   "source": [
    "## Marks\n",
    "\n",
    "**Total Marks earned:** \n",
    "\n",
    "**Score for Full Marks:**  70\n",
    "\n",
    "**Bonus marks:** 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f91420-ce72-4ac2-a88f-1e68366639f5",
   "metadata": {},
   "source": [
    "## Assignment Outline\n",
    "\n",
    "Throughout this course's assignment series, you will get the training needed to complete the final report. With each new assignment in this course, you will master the technical skills to implement analyses (Assignment 1), train reproducible research practices (Assignment 2), and learn key concepts regarding prediction modelling (Assignment 3).\n",
    "\n",
    "In Assignment 3, you will learn more about key concepts of prediction modelling:\n",
    "\n",
    "- Implementation of Basic Supervised Learning Methods (KNN and Linear Regression)\n",
    "\n",
    "- Prediction Error Sources (Underfitting vs. Overfitting)\n",
    "\n",
    "- Prediction Performance Measures\n",
    "\n",
    "- Predictive Performance Assessment Methods (Hold-Out Set)\n",
    "\n",
    "- Variable Selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06b753-52a7-4a1d-86e1-94db0363e644",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The goal of prediction modelling is to predict an outcome variable using other available variables (i.e. predictors) for new and previously unseen subjects/units. This makes prediction modelling desirable given its applicability to many different problems (e.g., medical patient outcomes, weather, image recognition, finances, etc.), and that is why machine learning has become a very popular discipline these past few decades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658079e-960f-4e54-b6c9-5da9e4d71968",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "**Objective**: We're going to apply linear regression modelling to the Ames City Iowa housing dataset and attempt to predict Sale Price using the following variables:\n",
    "\n",
    "- Land Slope\n",
    "- Lot Area\n",
    "- Overall Condition\n",
    "- First Floor Surface Area (Square Feet)\n",
    "- Building Type\n",
    "\n",
    "Please run this code before proceeding and examine the variables we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcfc92ec-841f-4a27-b463-d664b0ab2d42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m2925\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m75\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[31mchr\u001b[39m (40): MS_SubClass, MS_Zoning, Street, Alley, Lot_Shape, Land_Contour, Ut...\n",
      "\u001b[32mdbl\u001b[39m (35): Order, Lot_Frontage, Lot_Area, Year_Built, Year_Remod_Add, Mas_Vnr...\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Sale_Price</th><th scope=col>Land_Slope</th><th scope=col>Lot_Area</th><th scope=col>Overall_Cond</th><th scope=col>First_Flr_SF</th><th scope=col>Bldg_Type</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>215000</td><td>Gtl</td><td>31770</td><td>Average      </td><td>1656</td><td>OneFam</td></tr>\n",
       "\t<tr><td>105000</td><td>Gtl</td><td>11622</td><td>Above_Average</td><td> 896</td><td>OneFam</td></tr>\n",
       "\t<tr><td>172000</td><td>Gtl</td><td>14267</td><td>Above_Average</td><td>1329</td><td>OneFam</td></tr>\n",
       "\t<tr><td>244000</td><td>Gtl</td><td>11160</td><td>Average      </td><td>2110</td><td>OneFam</td></tr>\n",
       "\t<tr><td>189900</td><td>Gtl</td><td>13830</td><td>Average      </td><td> 928</td><td>OneFam</td></tr>\n",
       "\t<tr><td>195500</td><td>Gtl</td><td> 9978</td><td>Above_Average</td><td> 926</td><td>OneFam</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " Sale\\_Price & Land\\_Slope & Lot\\_Area & Overall\\_Cond & First\\_Flr\\_SF & Bldg\\_Type\\\\\n",
       " <dbl> & <chr> & <dbl> & <chr> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t 215000 & Gtl & 31770 & Average       & 1656 & OneFam\\\\\n",
       "\t 105000 & Gtl & 11622 & Above\\_Average &  896 & OneFam\\\\\n",
       "\t 172000 & Gtl & 14267 & Above\\_Average & 1329 & OneFam\\\\\n",
       "\t 244000 & Gtl & 11160 & Average       & 2110 & OneFam\\\\\n",
       "\t 189900 & Gtl & 13830 & Average       &  928 & OneFam\\\\\n",
       "\t 195500 & Gtl &  9978 & Above\\_Average &  926 & OneFam\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 6\n",
       "\n",
       "| Sale_Price &lt;dbl&gt; | Land_Slope &lt;chr&gt; | Lot_Area &lt;dbl&gt; | Overall_Cond &lt;chr&gt; | First_Flr_SF &lt;dbl&gt; | Bldg_Type &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 215000 | Gtl | 31770 | Average       | 1656 | OneFam |\n",
       "| 105000 | Gtl | 11622 | Above_Average |  896 | OneFam |\n",
       "| 172000 | Gtl | 14267 | Above_Average | 1329 | OneFam |\n",
       "| 244000 | Gtl | 11160 | Average       | 2110 | OneFam |\n",
       "| 189900 | Gtl | 13830 | Average       |  928 | OneFam |\n",
       "| 195500 | Gtl |  9978 | Above_Average |  926 | OneFam |\n",
       "\n"
      ],
      "text/plain": [
       "  Sale_Price Land_Slope Lot_Area Overall_Cond  First_Flr_SF Bldg_Type\n",
       "1 215000     Gtl        31770    Average       1656         OneFam   \n",
       "2 105000     Gtl        11622    Above_Average  896         OneFam   \n",
       "3 172000     Gtl        14267    Above_Average 1329         OneFam   \n",
       "4 244000     Gtl        11160    Average       2110         OneFam   \n",
       "5 189900     Gtl        13830    Average        928         OneFam   \n",
       "6 195500     Gtl         9978    Above_Average  926         OneFam   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(200)\n",
    "library(tidyverse)\n",
    "Ames_Data = read_csv(\"../data/ames.csv\")\n",
    "Ames_Data=Ames_Data[,c(\"Sale_Price\",\"Land_Slope\",\"Lot_Area\",\"Overall_Cond\",\"First_Flr_SF\",\"Bldg_Type\")]\n",
    "head(Ames_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8005e64-fd32-4ee8-92d0-01a1f49e0814",
   "metadata": {},
   "source": [
    "Next, we want to divide our dataset into a training set (to train the model) and a test set (to test the model's performance). Let's proceed by using 80% of the original dataset as our train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a917c0-c834-4356-9867-659af01e9e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Number of Observations in Training Set: 2340'"
      ],
      "text/latex": [
       "'Number of Observations in Training Set: 2340'"
      ],
      "text/markdown": [
       "'Number of Observations in Training Set: 2340'"
      ],
      "text/plain": [
       "[1] \"Number of Observations in Training Set: 2340\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'Number of Observations in Test Set: 585'"
      ],
      "text/latex": [
       "'Number of Observations in Test Set: 585'"
      ],
      "text/markdown": [
       "'Number of Observations in Test Set: 585'"
      ],
      "text/plain": [
       "[1] \"Number of Observations in Test Set: 585\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Train_SI = sample(1:nrow(Ames_Data), size=ceiling(0.8*nrow(Ames_Data)))\n",
    "Test_SI = which(!(1:nrow(Ames_Data) %in% Train_SI))\n",
    "Ames_Train = Ames_Data[Train_SI,]\n",
    "Ames_Test = Ames_Data[Test_SI,]\n",
    "paste(\"Number of Observations in Training Set: \", nrow(Ames_Train), sep=\"\")\n",
    "paste(\"Number of Observations in Test Set: \", nrow(Ames_Test), sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc3aef-86ac-4ab7-85fa-8869cfab7a34",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "Explain, in your own words, why have we decided to split the dataset into two parts **before** creating our prediction model?\n",
    "\n",
    "**Hint:** Think about the problem that happens if we don't split the dataset and use the entire dataset to both build and test our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e313f-3c5a-45b1-be6e-e7636e239674",
   "metadata": {},
   "source": [
    "(5 points) for providing the appropriate reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69e26b-6ed1-426d-b9c1-fffb0b92b322",
   "metadata": {},
   "source": [
    "**[Answer Here]** \n",
    "* If we don't split the dataset, the model would be built and trained too strictly and result in overfitting. This would result in good performance on the training data but poor generalisation to new unseen data. By splitting the data, we would ensure that the model predictive power is robust and not just tailored to a specific instance in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb54de1-20c0-47ac-9235-4367bf56009c",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "Let's fit a linear regression to model Sale Price in terms of the other variables listed above in the objective. Complete the following code (replace the `...` with appropriate code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b28ac537-bd7b-4c35-9d97-fdc8c12e072a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Sale_Price ~ Land_Slope + Lot_Area + Overall_Cond + \n",
       "    First_Flr_SF + Bldg_Type, data = Ames_Train)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-234977  -33943   -5897   28280  319981 \n",
       "\n",
       "Coefficients:\n",
       "                            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)               -5.533e+04  7.426e+03  -7.452 1.29e-13 ***\n",
       "Land_SlopeMod              1.206e+04  5.546e+03   2.175 0.029744 *  \n",
       "Land_SlopeSev             -4.603e+04  1.707e+04  -2.696 0.007073 ** \n",
       "Lot_Area                   1.024e+00  1.724e-01   5.937 3.34e-09 ***\n",
       "Overall_CondAverage        3.951e+04  3.061e+03  12.908  < 2e-16 ***\n",
       "Overall_CondBelow_Average -2.277e+04  6.453e+03  -3.529 0.000425 ***\n",
       "Overall_CondExcellent      4.241e+04  1.009e+04   4.204 2.73e-05 ***\n",
       "Overall_CondFair          -5.252e+04  8.559e+03  -6.137 9.88e-10 ***\n",
       "Overall_CondGood           7.597e+03  4.011e+03   1.894 0.058329 .  \n",
       "Overall_CondPoor          -3.359e+04  1.906e+04  -1.763 0.078083 .  \n",
       "Overall_CondVery_Good      1.536e+04  5.586e+03   2.750 0.006011 ** \n",
       "Overall_CondVery_Poor     -5.852e+04  2.212e+04  -2.646 0.008203 ** \n",
       "First_Flr_SF               1.182e+02  3.250e+00  36.359  < 2e-16 ***\n",
       "Bldg_TypeOneFam            6.784e+04  5.897e+03  11.504  < 2e-16 ***\n",
       "Bldg_TypeTwnhs             7.149e+04  8.510e+03   8.401  < 2e-16 ***\n",
       "Bldg_TypeTwnhsE            6.899e+04  6.965e+03   9.906  < 2e-16 ***\n",
       "Bldg_TypeTwoFmCon          3.453e+04  9.272e+03   3.725 0.000200 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 53320 on 2323 degrees of freedom\n",
       "Multiple R-squared:  0.5313,\tAdjusted R-squared:  0.5281 \n",
       "F-statistic: 164.6 on 16 and 2323 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SalePrice_Model = lm(Sale_Price ~ Land_Slope + Lot_Area + Overall_Cond + First_Flr_SF + Bldg_Type, data=Ames_Train)\n",
    "summary(SalePrice_Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0320aa-2304-403b-9d06-f1e5adc4fe5b",
   "metadata": {},
   "source": [
    "Intepreting Model Output: \n",
    "- What is the coefficient value for First Floor Surface Area?\n",
    "- Which coefficient is not statistically signficant at the 5% level?\n",
    "- What is the standard error for the coefficient corresponding to \"Poor\" Overall Condition?\n",
    "\n",
    "**Hint:** Recall that \"e+XX\" indicates how many decimal places to move the decimal to the right. So 5.000e+04 is 50000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d159a1d2-1a88-4f3c-a638-4d269ab681e4",
   "metadata": {},
   "source": [
    "(10 points) - 4 points for correct code completion, 2 points for each question answered correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5fd7da-1f81-4e64-99df-cf57da138e2f",
   "metadata": {},
   "source": [
    "**[Answer Here]**\n",
    "* 118.2\n",
    "* Overall_CondGood (p-value: 0.058329) and Overall_CondPoor (p-value: 0.078083)\n",
    "* 19060"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5900d015-1f32-4d07-a59e-c5acf8de677e",
   "metadata": {},
   "source": [
    "### Part C\n",
    "\n",
    "Now we want to test the predictive performance of this linear regression model. What is an appropriate choice of performance measures to use here?\n",
    "\n",
    "**A)** Sensitivity\n",
    "\n",
    "**B)** Mean Squared Error\n",
    "\n",
    "**C)** Krippendorf's Alpha\n",
    "\n",
    "**D)** Area Under the Curve\n",
    "\n",
    "**E)** Matthew's Correlation Coefficient\n",
    "\n",
    "**Hint:** Think about the type of outcome we are predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11457ee-5e68-436b-bc2a-ae50a6f2905a",
   "metadata": {},
   "source": [
    "(5 points) for correct choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd6a81-a278-4158-aeea-20e98ef08ee4",
   "metadata": {},
   "source": [
    "**[Answer Here]**\n",
    "* B\n",
    "* Since Sale_Price is a continuous variable, it would be suitable to use a MSE to test the predictive performance of this linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a13fa4-dcee-41a4-86e5-6785b834fd64",
   "metadata": {},
   "source": [
    "### Part D\n",
    "\n",
    "Let's choose to use mean absolute error (MAE) to assess the performance of our model. Complete the below code to compute this measure for both the training set and the test set (replace the `...` with appropriate code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0dcb1ee-1666-4864-b765-026a8d64d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingSetError = sum(abs(predict(SalePrice_Model, Ames_Train) - Ames_Train$Sale_Price))/nrow(Ames_Train)\n",
    "TestSetError = sum(abs(predict(SalePrice_Model, Ames_Test) - Ames_Test$Sale_Price))/nrow(Ames_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e4559-b98f-42dc-b050-0b7adfd6cc90",
   "metadata": {},
   "source": [
    "**Hint:** Recall the expression for mean absolute error (MAE). What absolute differences are we taking and summing together?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a11f837-423d-46e6-a46f-0e9779c69208",
   "metadata": {},
   "source": [
    "(5 points) for correct code completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074642b-29cf-469b-b3af-f40ddac12810",
   "metadata": {},
   "source": [
    "### Part E\n",
    "\n",
    "In an effort to improve the linear regression model's performance, a student includes many of the other available variables as predictors in the model. The student also tries out many different combinations of those predictors and selects a final model based on the best performance on the test set. Name at least one possible problem with this modelling approach, if we assume to use the same above modelling procedure (i.e. every new model is repeatedly built using the same code as above, only the model predictors change). Please explain in a couple of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf742b-e270-40fb-8d89-46625164a4a7",
   "metadata": {},
   "source": [
    "(5 points, 5 bonus) - For naming one problem correctly, bonus marks as well for naming two problems correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb3162-6e66-440a-a4f5-731f72900adb",
   "metadata": {},
   "source": [
    "**[Answer Here]** \n",
    "* One possible problem is overfitting the model to the test set. This could result in the model performing extremely well on the test data but not be optimised for new unseen data. \n",
    "* Another problem is data snooping bias. This occurs when the test data influences the model selection process, leading to overly optimistic performance estimates. A seperate validation set for model selection is needed to keep the model from having this bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c98046-951b-4706-bd81-58ec100a28e8",
   "metadata": {},
   "source": [
    "### Part F\n",
    "\n",
    "Let's assume that the above student is interested in performing model selection (i.e. selecting predictors), and wants to select the final model that minimizes the performance measure they're using. How can the student update their modelling pipeline so that predictive performance of the selected final model can still be assessed accurately? \n",
    "\n",
    "Choose **all** that apply:\n",
    "\n",
    "**A)** Use a training/validation/test split of the dataset\n",
    "\n",
    "**B)** Use a bootstrap validation\n",
    "\n",
    "**C)** Use a repeated cross-validation\n",
    "\n",
    "**D)** Use a nested cross-validation\n",
    "\n",
    "**E)** Use a LOESS instead of linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a987b8c-596b-40d6-93c4-70edde75f01b",
   "metadata": {},
   "source": [
    "(5 points) for correct choices selected, -2 marks for each incorrect choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31ca8af-7aa4-4660-9bed-ebf9e34e6dbf",
   "metadata": {},
   "source": [
    "**[Answer Here]**\n",
    "* A, C and D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4262aa2-8080-4ab8-93f6-ed9b71e8fc2f",
   "metadata": {},
   "source": [
    "### Part G - (BONUS)\n",
    "\n",
    "To tackle the problem of model selection, the student decides to use a LASSO model instead of a linear regression. What is the problem with using a LASSO model to build a prediction model here? Assume that the correct predictive performance assessment pipeline is being used.\n",
    "\n",
    "**Hint:** Look at the variable types of the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4462dade-c602-45ac-b0c4-67cd985cc7af",
   "metadata": {},
   "source": [
    "(5 bonus) for correct identification of problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b0f89-b06c-4d65-9557-076b8c49e1b7",
   "metadata": {},
   "source": [
    "**[Answer Here]**\n",
    "* One problem using LASSO is that it may have difficulties handling categorical variables, leading to incorrect or suboptimal selection of predictors. In the dataset, Land_Slope, Overall_Cond and Bldg_Type are all categorical variables. LASSO requires a numerical input, thus these categorical variables would need to be transformed into a suitable format to be able to use LASSO properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957267e3-8887-4276-b7ad-7dcd7046d90f",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "**Objective**: Next, we're going to now use k-nearest neighbours (k=9) to predict Central Air Conditioning (Y/N) using the following variables:\n",
    "\n",
    "- Ground Living Area (Square Feet)\n",
    "- Lot Area\n",
    "- Total Basement Surface Area (Square Feet)\n",
    "- First Floor Surface Area (Square Feet)\n",
    "- Garage Area\n",
    "\n",
    "Please run this code before proceeding and examine the variables we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28a8e48f-ceef-41c9-ad2c-027b5a28f9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m2925\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m75\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[31mchr\u001b[39m (40): MS_SubClass, MS_Zoning, Street, Alley, Lot_Shape, Land_Contour, Ut...\n",
      "\u001b[32mdbl\u001b[39m (35): Order, Lot_Frontage, Lot_Area, Year_Built, Year_Remod_Add, Mas_Vnr...\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Gr_Liv_Area</th><th scope=col>Lot_Area</th><th scope=col>Total_Bsmt_SF</th><th scope=col>First_Flr_SF</th><th scope=col>Garage_Area</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1656</td><td>31770</td><td>1080</td><td>1656</td><td>528</td></tr>\n",
       "\t<tr><th scope=row>2</th><td> 896</td><td>11622</td><td> 882</td><td> 896</td><td>730</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>1329</td><td>14267</td><td>1329</td><td>1329</td><td>312</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>2110</td><td>11160</td><td>2110</td><td>2110</td><td>522</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>1629</td><td>13830</td><td> 928</td><td> 928</td><td>482</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>1604</td><td> 9978</td><td> 926</td><td> 926</td><td>470</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 5\n",
       "\\begin{tabular}{r|lllll}\n",
       "  & Gr\\_Liv\\_Area & Lot\\_Area & Total\\_Bsmt\\_SF & First\\_Flr\\_SF & Garage\\_Area\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 1656 & 31770 & 1080 & 1656 & 528\\\\\n",
       "\t2 &  896 & 11622 &  882 &  896 & 730\\\\\n",
       "\t3 & 1329 & 14267 & 1329 & 1329 & 312\\\\\n",
       "\t4 & 2110 & 11160 & 2110 & 2110 & 522\\\\\n",
       "\t5 & 1629 & 13830 &  928 &  928 & 482\\\\\n",
       "\t6 & 1604 &  9978 &  926 &  926 & 470\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 5\n",
       "\n",
       "| <!--/--> | Gr_Liv_Area &lt;dbl&gt; | Lot_Area &lt;dbl&gt; | Total_Bsmt_SF &lt;dbl&gt; | First_Flr_SF &lt;dbl&gt; | Garage_Area &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 1 | 1656 | 31770 | 1080 | 1656 | 528 |\n",
       "| 2 |  896 | 11622 |  882 |  896 | 730 |\n",
       "| 3 | 1329 | 14267 | 1329 | 1329 | 312 |\n",
       "| 4 | 2110 | 11160 | 2110 | 2110 | 522 |\n",
       "| 5 | 1629 | 13830 |  928 |  928 | 482 |\n",
       "| 6 | 1604 |  9978 |  926 |  926 | 470 |\n",
       "\n"
      ],
      "text/plain": [
       "  Gr_Liv_Area Lot_Area Total_Bsmt_SF First_Flr_SF Garage_Area\n",
       "1 1656        31770    1080          1656         528        \n",
       "2  896        11622     882           896         730        \n",
       "3 1329        14267    1329          1329         312        \n",
       "4 2110        11160    2110          2110         522        \n",
       "5 1629        13830     928           928         482        \n",
       "6 1604         9978     926           926         470        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Ames_Y_Data\n",
       "   Y    N \n",
       "2729  196 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(300)\n",
    "library(tidyverse)\n",
    "library(class)\n",
    "Ames_Data = read_csv(\"../data/ames.csv\")\n",
    "Ames_X_Data=as.data.frame(Ames_Data[,c(\"Gr_Liv_Area\",\"Lot_Area\",\"Total_Bsmt_SF\",\"First_Flr_SF\",\"Garage_Area\")])\n",
    "Ames_Y_Data=as.vector(data.frame(Ames_Data)[,\"Central_Air\"])\n",
    "Ames_Y_Data=factor(Ames_Y_Data, levels=c(\"Y\", \"N\"))\n",
    "head(Ames_X_Data)\n",
    "table(Ames_Y_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a86496-7065-4ab2-975b-44bf49ebb18e",
   "metadata": {},
   "source": [
    "And then again, we want to divide our dataset into a training set (to train the model) and a test set (to test the model's performance). Let's proceed by using 80% of the original dataset as our train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d8874c0-dd27-40c8-8fb4-363c612e7b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Number of Observations in Training Set: 2340'"
      ],
      "text/latex": [
       "'Number of Observations in Training Set: 2340'"
      ],
      "text/markdown": [
       "'Number of Observations in Training Set: 2340'"
      ],
      "text/plain": [
       "[1] \"Number of Observations in Training Set: 2340\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'Number of Observations in Test Set: 585'"
      ],
      "text/latex": [
       "'Number of Observations in Test Set: 585'"
      ],
      "text/markdown": [
       "'Number of Observations in Test Set: 585'"
      ],
      "text/plain": [
       "[1] \"Number of Observations in Test Set: 585\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Train_SI = sample(1:nrow(Ames_X_Data), size=ceiling(0.8*nrow(Ames_X_Data)))\n",
    "Test_SI = which(!(1:nrow(Ames_X_Data) %in% Train_SI))\n",
    "\n",
    "Ames_X_Train = Ames_X_Data[Train_SI,]\n",
    "Ames_X_Test = Ames_X_Data[Test_SI,]\n",
    "\n",
    "Ames_Y_Train = Ames_Y_Data[Train_SI]\n",
    "Ames_Y_Test = Ames_Y_Data[Test_SI]\n",
    "\n",
    "paste(\"Number of Observations in Training Set: \", nrow(Ames_X_Train), sep=\"\")\n",
    "paste(\"Number of Observations in Test Set: \", nrow(Ames_X_Test), sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21adc15b-fd4a-4b8a-93e2-ce346a97c1e3",
   "metadata": {},
   "source": [
    "And at last, we make predictions using k-nearest neighbours (selecting k=9) on our test set using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d88bf5-0402-4ab1-8d76-03030f662409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNN_Test_Preds\n",
       "  Y   N \n",
       "579   6 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "KNN_Test_Preds = knn(Ames_X_Train, Ames_X_Test, Ames_Y_Train, k=9)\n",
    "table(KNN_Test_Preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33fb78-7546-4f4a-83bc-265100c31a7f",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "Build the confusion matrix and then calculate the sensitivity, specificity, positive predictive value and negative predictive value by completing the following code (replace the `...` with appropriate code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c870c6b-e8bd-48c6-9b88-77c65feb5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Con_Mat = table(Ames_Y_Test, KNN_Test_Preds)\n",
    "\n",
    "Sensitivity = Con_Mat[1,1]/(Con_Mat[1, 1]+Con_Mat[2, 1])\n",
    "Specificity = Con_Mat[2,2]/(Con_Mat[2, 2]+Con_Mat[1, 2])\n",
    "\n",
    "PPV = Con_Mat[1,1]/(Con_Mat[1, 1]+Con_Mat[1, 2])\n",
    "NPV = Con_Mat[2,2]/(Con_Mat[2, 2]+Con_Mat[2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd9b87a-86e7-4d4e-b2a2-f11ce988337a",
   "metadata": {},
   "source": [
    "(15 points) - 3 points for each correct line of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30dcb0-42a3-46b3-b63c-5462590bfb70",
   "metadata": {},
   "source": [
    "(5 bonus) - Use the appropriate functions in caret to compute these same measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfafd991-6cfa-4d01-b7ab-09bf13a538c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   Y   N\n",
       "         Y 546   1\n",
       "         N  33   5\n",
       "                                          \n",
       "               Accuracy : 0.9419          \n",
       "                 95% CI : (0.9197, 0.9594)\n",
       "    No Information Rate : 0.9897          \n",
       "    P-Value [Acc > NIR] : 1               \n",
       "                                          \n",
       "                  Kappa : 0.2133          \n",
       "                                          \n",
       " Mcnemar's Test P-Value : 1.058e-07       \n",
       "                                          \n",
       "            Sensitivity : 0.9430          \n",
       "            Specificity : 0.8333          \n",
       "         Pos Pred Value : 0.9982          \n",
       "         Neg Pred Value : 0.1316          \n",
       "             Prevalence : 0.9897          \n",
       "         Detection Rate : 0.9333          \n",
       "   Detection Prevalence : 0.9350          \n",
       "      Balanced Accuracy : 0.8882          \n",
       "                                          \n",
       "       'Positive' Class : Y               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Bonus\n",
    "\n",
    "library(caret)\n",
    "\n",
    "confusion_matrix = confusionMatrix(Ames_Y_Test, KNN_Test_Preds)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc3bca-faf9-4056-a1c7-b2c498b82ff6",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "Explain in one or two sentences why using the \"accuracy\" measure for this problem is likely a bad idea.\n",
    "\n",
    "**Hint:** Examine the outcome more closely.\n",
    "\n",
    "(5 points) for correct reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a8bef-6c88-4acb-91db-4c502417e55c",
   "metadata": {},
   "source": [
    "**[Answer Here]**\n",
    "* As the outcome variable (Central Air Conditioning) is highly imbalanced, with majority of the instances being 'Y' (2729) and minority being 'N' (196). Due to this huge difference in instances, the accuracy calculated may be misleading. As a high accuracy can still be achieved by predicting the majority class without capturing the minority class, thus failing to measure the model's true performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df92b4-a25c-4c38-8337-7d6b7ce948c5",
   "metadata": {},
   "source": [
    "### Part C\n",
    "\n",
    "We can see the results from the confusion matrix that prediction of the \"N\" class using our k-nearest neighbours approach is very poor (only 5 out of 38 are correctly predicted), likely because the class is rare. Technically speaking, we can't perform any further modelling now that we've seen the test error (unless we have additional unused data leftover), because we otherwise risk overfitting (showing the importance of planning out your prediction modelling). \n",
    "\n",
    "However, for pedagogical purposes, let's assume we anticipated a problem with predicting this minority class in our data **before** we started modelling. Name a technique we could have used on the data to potentially improve predictive performance on the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c39d646-a65a-4f25-8362-2ab6f2ef35da",
   "metadata": {},
   "source": [
    "(5 points, 5 bonus) for correctly identifying one of the methods (and explaining what it does), another 5 bonus for explaining which is better to use for this specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833f4149-1740-4015-ab18-7cce44600e6c",
   "metadata": {},
   "source": [
    "**[Answer Here]**\n",
    "* Upsampling can be used to potentially improve the predictive performance on the minority class. By randomly duplicating observations from the minority class until the majority and minority is even or close to even, it would help alleviate the severe class imbalance problem, thus potentially improve predictive performance on the minority class.\n",
    "* Downsampling can also be used potentially improve the predictive performance on the minority class. By randomly removing observations from the majority class until the majority and minority is even or close to even, it would help alleviate the severe class imbalance problem, thus potentially improve predictive performance on the minority class.\n",
    "* Upsampling is better to use for this specific problem, as the minority class is very small compared to the majority class. Randomly removing observations from the majority class until majority and minority class are even (downsampling), would result in a major loss of information and potentially affect the predictive performance of the model. Thus, upsampling would be more suitable in this specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76bd85-5685-404e-82cd-5f1a8305d842",
   "metadata": {},
   "source": [
    "### Part D\n",
    "\n",
    "Here is the code for the application of this technique from Part C, as well as running k-nearest neighbours (k=9) once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2d55055-7dac-482d-a55f-cbcf757973e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNN_Test_Preds\n",
       "  Y   N \n",
       "433 152 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "           KNN_Test_Preds\n",
       "Ames_Y_Test   Y   N\n",
       "          Y 421 126\n",
       "          N  12  26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_Count = table(Ames_Y_Train)[1]\n",
    "N_Count = table(Ames_Y_Train)[2]\n",
    "Diff = Y_Count-N_Count\n",
    "N_Inds = which(Ames_Y_Train==\"N\")\n",
    "Repeats = sample(N_Inds, Diff, replace=TRUE)\n",
    "\n",
    "Ames_X_Train_New = rbind(Ames_X_Train, Ames_X_Train[Repeats,])\n",
    "Ames_Y_Train_New = c(Ames_Y_Train, Ames_Y_Train[Repeats])\n",
    "\n",
    "KNN_Test_Preds = knn(Ames_X_Train_New, Ames_X_Test, Ames_Y_Train_New, k=9)\n",
    "table(KNN_Test_Preds)\n",
    "table(Ames_Y_Test, KNN_Test_Preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce27f2-bae0-4cad-be9c-c739f0b85480",
   "metadata": {},
   "source": [
    "We can see that the model is better at predicting the minority class, but now prediction of the majority class suffers (124 \"Y\" observations being predicted as \"N\"). This highlights the problem with k-nearest neighbours handling imbalanced outcome data. \n",
    "\n",
    "The `knn` function from the `class` package in R only takes the k-nearest neighbours and assigns predictions based on majority vote (ties are broken **randomly**). So if 5 of the neighbours are \"N\" and 4 of the neighbours are \"Y\", then the prediction point will be classified as \"N\", regardless of how close the \"Y\" neighbours are. \n",
    "\n",
    "How can we use the distances of the k-nearest neighbours of any given prediction point to potentially improve predictions? This can be a general explanation, no need for mathematical equations/expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a2fb12-f90f-47bd-98da-d403a32186a6",
   "metadata": {},
   "source": [
    "(5 points) for correct explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb2a9b-a95e-45ab-9885-8c9d20eab422",
   "metadata": {},
   "source": [
    "**[Answer Here]**\n",
    "* We can use the distances of the k-nearest neighbours to do distance-based weighting. Instead of using the majority vote among the k-nearest neighbours, we can weight the votes based on the distances of the neighbours. Closer neighbours can have a greater influence on the prediction than further neighbours. So for each prediction, we can assign a weight to each of the k-nearest neighbours based on their distance from the point being predicted (closer have more influence than those further away), we then used the weighted votes (for each class, multiply votes by respective weight) to make the final prediction. This allows the prediction to be more balanced as it will be less affected by skewed distant neighbours and allows the model to be more sensitive to local patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115637e8-72b0-4079-94cc-84bbc27be016",
   "metadata": {},
   "source": [
    "### Part E\n",
    "\n",
    "As you may have noticed, we have not conducted any hyperparameter tuning of the k hyperparameter, instead we've always set k=9 (an initial, hopefully reasonable choice). However, let's consider a modelling approach where we do incorporate hyperparameter tuning to select and test our final model. Specifically, let's say that we're going to perform 20-fold cross-validation on our training set, repeated 100 times, to ultimately select the hyperparameter k with the best performance (let's say average of sensitivity and specificity).\n",
    "\n",
    "What is one possible concern with this approach (with respect to **specifically** the outcome class variable), and how can this problem be addressed?\n",
    "\n",
    "**Hint:** What's going to happen to the outcome distribution of each fold? Will each fold always have \"N\" observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da624fcf-161d-4ec9-a336-c602079be978",
   "metadata": {},
   "source": [
    "(5 points) for correctly identifying the problem and how it can be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2858d6e4-9dde-47b5-8db9-1b72bee4cdd2",
   "metadata": {},
   "source": [
    "**[Answer Here]**\n",
    "* Problem: Each fold may not contain a representative sample of the minority class, leading to inconsistent and unreliable performance estimates for the minority class, skewing the overall evaluation of the model's performance such as sensitivity and specificity.\n",
    "* Solution: Use stratified k-fold cross-validation, where each fold maintains the original class distribution. This ensures that each fold contains a proportionate number of observations for each class, preserving the balance between the majority and minority classes within each fold. Stratification helps provide a more accurate and consistent evaluation of the model’s performance across different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085656aa-969a-46a0-a7dd-4fa3b56d4b30",
   "metadata": {},
   "source": [
    "## Upload your work from Assignment 3\n",
    "\n",
    "- Each student will upload the Jupiter Notebook on Canvas Course 2: https://canvas.ubc.ca/courses/155856\n",
    "\n",
    " `Assignment_3C2_[team #]_[student name].ipynb`\n",
    "\n",
    "EXAMPLE: `Assignment_3C2_Team1_Nikolas_Krstic.ipynb`\n",
    "\n",
    "- Please write at the title who was responsible for writing each paragraph. \n",
    "\n",
    "Upload the word document on Canvas Course 2 under:\n",
    "`Assignments -> Assignment 3` \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
